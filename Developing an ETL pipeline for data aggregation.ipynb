{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e06910dd",
   "metadata": {
    "id": "e06910dd"
   },
   "source": [
    "# Разработка ETL-пайплайна для агрегации данных о поездках\n",
    "\n",
    "\n",
    "- Автор: Коростин Никита\n",
    "- Дата: 05.11.2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4149134c",
   "metadata": {
    "id": "4149134c"
   },
   "source": [
    "## Описание проекта\n",
    "\n",
    "Вы работаете аналитиком данных. Каждый день компания обрабатывает миллионы поездок, оплаченных разными способами. Чтобы финансовая и продуктовая команды регулярно получали актуальные данные о выручке и поведении пассажиров, коллеги решили настроить автоматическую обработку этих данных.\n",
    "\n",
    "Ваша задача — построить витрину данных, которая будет агрегировать информацию о поездках по каждому способу оплаты. Для этого нужно написать PySpark-скрипт, который рассчитает ключевые показатели: количество поездок, среднюю стоимость поездки, средние чаевые и суммарную выручку по каждому типу оплаты.\n",
    "\n",
    "Чтобы процесс был полностью автоматическим и не зависел от ручных запусков, необходимо создать DAG в Airflow. Этот DAG должен ежедневно:\n",
    "\n",
    "* проверять наличие новых файлов с данными;\n",
    "\n",
    "* запускать Spark-задачу;\n",
    "\n",
    "* формировать обновлённую итоговую таблицу.\n",
    "\n",
    "Эта таблица станет основой для финансовых отчётов и аналитических дашбордов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728986ea",
   "metadata": {
    "id": "728986ea"
   },
   "source": [
    "## Описание данных\n",
    "\n",
    "Таблица `taxi_data` содержит данные об активности пользователей и состоит из следующих полей:\n",
    "\n",
    "* `taxi_id` — идентификатор водителя;\n",
    "\n",
    "* `trip_start_timestamp` — время начала поездки;\n",
    "\n",
    "* `trip_end_timestamp` — время окончания поездки;\n",
    "\n",
    "* `trip_seconds` — длительность поездки в секундах;\n",
    "\n",
    "* `trip_miles` — дистанция поездки;\n",
    "\n",
    "* `fare` — стоимость поездки;\n",
    "\n",
    "* `tips` — размер чаевых;\n",
    "\n",
    "* `trip_total` — общая стоимость поездки: стоимость поездки + чаевые + комиссия;\n",
    "\n",
    "* `payment_type` — способ оплаты."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fecaf95",
   "metadata": {
    "id": "4fecaf95"
   },
   "source": [
    "## Что нужно сделать\n",
    "\n",
    "В проекте вам предстоит автоматизировать подготовку витрины данных по поездкам:\n",
    "\n",
    "1. Сначала необходимо написать Spark-скрипт, который будет обрабатывать данные о поездках и агрегировать показатели по способам оплаты `payment_type`. Понадобится рассчитать несколько показателей:\n",
    "\n",
    "* количество поездок, которое показывает общий спрос и загрузку сервиса;\n",
    "* среднюю стоимость `fare`, которое отражает уровень среднего чека поездки;\n",
    "* средние чаевые `tips` — индикатор удовлетворённости клиентов и мотивации водителей;\n",
    "* суммарную выручку `trip_total` — ключевой показатель дохода компании.\n",
    "\n",
    "Все результаты должны собираться в одну итоговую таблицу `taxi_payment_summary`. После этого таблицу нужно записать в ClickHouse с помощью JDBC-драйвера.\n",
    "\n",
    "2. Далее вам понадобится настроить DAG в Airflow. DAG должен запускаться ежедневно. Перед запуском он проверяет наличие файла с данными за нужную дату в S3-хранилище и только после появления файла запускает Spark-задачу.\n",
    "\n",
    "Когда всё будет готово, перенесите свой код и результаты в шаблон для ревьюеров в следующем уроке. Учтите, что этот шаблон служит только для передачи решения — запустить в нём код не получится."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WGKIjeRdzV0U",
   "metadata": {
    "id": "WGKIjeRdzV0U"
   },
   "source": [
    "## Шаг 1. Настройте Spark-агрегацию"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QjNj1KvlzXu2",
   "metadata": {
    "id": "QjNj1KvlzXu2"
   },
   "source": [
    "Ваши данные для подключения к DBeaver:\n",
    "*   Имя пользователя — (скрыто)\n",
    "*   Пароль — (скрыто)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7d21d6",
   "metadata": {
    "id": "6f7d21d6"
   },
   "source": [
    "Данные хранятся в формате Parquet, поэтому для чтения используйте метод `spark.read.parquet()`. Это быстрее и надёжнее, чем CSV.\n",
    "\n",
    "Сгруппируйте данные по полю `payment_type` и рассчитайте четыре показателя:\n",
    "\n",
    "* количество поездок — `count(*)`;\n",
    "* среднюю стоимость — `avg(...)`;\n",
    "* средние чаевые — `avg(...)`;\n",
    "* суммарную выручку — `sum(...)`.\n",
    "\n",
    "Так получится витрина для анализа информации по каждому способу оплаты. После этого настройте запись полученной таблицы в ClickHouse. Проверьте, что всё работает, и переходите в шагу 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tbbutSRTECeT",
   "metadata": {
    "id": "tbbutSRTECeT"
   },
   "outputs": [],
   "source": [
    "#filename=my_spark_job.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import sys\n",
    "\n",
    "# Создаём Spark-сессию и при необходимости добавляем конфигурации\n",
    "spark = SparkSession.builder.appName(\"myAggregateTest\").config(\"fs.s3a.endpoint\", \"storage.yandexcloud.net\").getOrCreate()\n",
    "\n",
    "# Указываем порт и параметры кластера ClickHouse\n",
    "jdbcPort = 8443\n",
    "jdbcHostname = \"(скрыто)\"\n",
    "username = \"da_20250922_ac1634726c\"\n",
    "password = \"(скрыто)\"\n",
    "jdbcDatabase = \"playground_\" + username\n",
    "jdbcUrl = f\"jdbc:clickhouse://{jdbcHostname}:{jdbcPort}/{jdbcDatabase}?ssl=true\"\n",
    "\n",
    "df = spark.read.parquet(\"s3a://da-plus-dags/project_04/taxi_data.parquet\")\n",
    "\n",
    "summary_df = df.groupBy(\"payment_type\").agg(\n",
    "    F.count(\"*\").alias(\"trip_count\"),\n",
    "    F.avg(\"fare\").alias(\"avg_fare\"),\n",
    "    F.avg(\"tips\").alias(\"avg_tips\"),\n",
    "    F.sum(\"trip_total\").alias(\"total_revenue\")\n",
    ")\n",
    "\n",
    "# Запись результата в ClickHouse\n",
    "summary_df.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbcUrl) \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"dbtable\", \"taxi_payment_summary\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e82e2",
   "metadata": {
    "id": "956e82e2"
   },
   "source": [
    "## Шаг 2. Настройте DAG\n",
    "\n",
    "Дату в качестве параметра передавать не нужно. В DAG используйте `S3KeySensor`, чтобы дождаться появления файла в S3. После этого запускайте `DataprocCreatePysparkJobOperator`, передав путь к вашему скрипту. Дополнительный класс-оператор создавать не требуется."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ieU9hC2Aznes",
   "metadata": {
    "id": "ieU9hC2Aznes"
   },
   "source": [
    "Ваши данные для подключения к Airflow:\n",
    "*   IP — (скрыто)\n",
    "*   Имя пользователя — da_20250922_ac1634726c\n",
    "*   Пароль — (скрыто)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g5Lu23KBESh9",
   "metadata": {
    "id": "g5Lu23KBESh9"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from airflow import DAG\n",
    "from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor\n",
    "from airflow.providers.yandex.operators.dataproc import DataprocCreatePysparkJobOperator\n",
    "\n",
    "DAG_ID = \"taxi_payment_processing\"\n",
    "\n",
    "with DAG(\n",
    "    dag_id=DAG_ID,\n",
    "    start_date=datetime(2025, 1, 1),\n",
    "    schedule_interval=\"@daily\",\n",
    "    catchup=False\n",
    ") as dag:\n",
    "    wait_for_input = S3KeySensor(\n",
    "        task_id='wait_for_data',\n",
    "        bucket_name='da-plus-dags',\n",
    "        bucket_key='project_04/taxi_data.parquet',\n",
    "        aws_conn_id='s3',\n",
    "        poke_interval=300,\n",
    "        timeout=3600,\n",
    "    )\n",
    "\n",
    "    run_pyspark = DataprocCreatePysparkJobOperator(\n",
    "        task_id=\"run_pyspark_job\",\n",
    "        name=\"taxi_payment_analysis\",\n",
    "        cluster_id=\"(скрыто)\",\n",
    "        main_python_file_uri=\"s3a://da-plus-dags/da_20250922_ac1634726c/jobs/my_spark_job.py\"\n",
    "    )\n",
    "\n",
    "    wait_for_input >> run_pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6953e84c",
   "metadata": {
    "id": "6953e84c"
   },
   "source": [
    "## Шаг 3. Запустите DAG с помощью Airflow UI\n",
    "\n",
    "Теперь можно переходить к запуску. Нажмите кнопку «Проверить», подождите 5 минут и снова нажмите её. Вам будут показаны данные для входа в веб-интерфейс Airflow. В интерфейсе найдите ваш DAG и запустите его.\n",
    "\n",
    "Проверьте, что DAG выполнился, а результат соответствует ожиданиям. Если всё получилось — поздравляем, проект завершён!"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 106,
    "start_time": "2025-11-06T10:05:06.359Z"
   }
  ],
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
